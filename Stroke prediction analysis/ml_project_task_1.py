# -*- coding: utf-8 -*-
"""ML_Project_Task_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11mtS1QyUM3tVUa4XAmn4x0OdOPk7OmWW

# **Importing needed libraries**
"""

#EDA
import matplotlib.colors as colors
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import matplotlib.gridspec as grid_spec
from imblearn.over_sampling import SMOTE
import plotly.express as px
#preprocessing
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split,cross_val_score, cross_validate
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score
import pandas as pd
import numpy as np
from sklearn.impute import KNNImputer,SimpleImputer
from pandas_profiling import ProfileReport
import seaborn as sns


#Manipulation
from sklearn.impute import KNNImputer, SimpleImputer
from sklearn.compose import make_column_transformer
from imblearn.pipeline import make_pipeline
from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold
from imblearn.under_sampling import RandomUnderSampler

#Models
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from collections import Counter
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV


from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.metrics import precision_recall_fscore_support, classification_report, confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay

"""# **Data exploration**"""

df = pd.read_csv('healthcare-dataset-stroke-data.csv')
df.head(5)
df.drop('id', axis = 1)

"""## **Number of stroke patients vs Healthy individuals in Dataset**"""

fig = px.histogram(df, x='stroke', color='stroke')
fig.update_layout(bargap=0.2)
fig.update_layout(showlegend=False)
fig.show()

"""## **Gender Distribution of samples**"""

fig = px.histogram(df, x='gender', color='gender')
fig.update_layout(bargap=0.2)
fig.update_layout(showlegend=False)
fig.show();

df['gender'].value_counts()

"""**The row containing the "Other" Gender will be removed for the remainder of the analysis**"""

df = df.drop(df[df['gender'] == 'Other'].index)

fig = px.histogram(df, x='gender', color='gender')
fig.update_layout(bargap=0.2)
fig.update_layout(showlegend=False)
fig.show();

"""## **Age distributions**

**Age distribution for stroke patients and healthy individuals**
"""

fig = px.histogram(df, x='age', color='stroke')
fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))
fig.show()

"""More Female cases than males are present in the dataset.

**Age distribution of Stroke and Non-stroke patients divided by gender**
"""

fig = px.box(df,
                y='age',
                x='stroke',
                color='gender',
                points='all')
fig.update_layout(boxgroupgap=0.4)
fig.show()

"""There are two outliers in the female stroke patient subset"""

df[(df['stroke'] == 1) & (df['age'] < 30)]

"""The female at 1.32 years old has no record of BMI and no record of heart disease or hypertension. Therefore her record will be removed for remaining analysis"""

df = df.drop(df[(df['stroke'] == 1) & (df['age'] == 1.32)].index)

df[(df['stroke'] == 1) & (df['age'] < 30)]

"""## **Average Glucose Level Distributions**

**Average Glucose Level distribution for stroke patients and healthy individuals**
"""

fig = px.histogram(df, x='avg_glucose_level', color='stroke')
fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))
fig.show()

"""**Average Glucose level distribution of Stroke and Non-stroke patients divided by gender**"""

fig = px.box(df,
                y='avg_glucose_level',
                x='stroke',
                color='gender',
                points='all')
fig.update_layout(boxgroupgap=0.4)
fig.show()

"""## **BMI Distributions**

**BMI distribution of stroke patients and healthy individuals**
"""

fig = px.histogram(df, x='bmi', color='stroke')
fig.update_layout(margin=dict(l=20, r=20, t=20, b=20))
fig.show()

"""**BMI distribution of Stroke and Non-stroke patients divided by gender**"""

fig = px.box(df,
                y='bmi',
                x='stroke',
                color='gender',
                points='all')
fig.update_layout(boxgroupgap=0.4)
fig.show()

"""## **Hypertension Distributions**

**Hypertension distribution of both stroke and non-stroke patients**
"""

fig = px.histogram(df, x='hypertension', color='stroke', barmode='group')
fig.update_layout(bargap=0.2)
fig.show();

"""## **Heart Disease Distribution**

**Heart Disease distribution of both stroke and non-stroke patients**
"""

fig = px.histogram(df, x='heart_disease', color='stroke', barmode='group')
fig.update_layout(bargap=0.2)
fig.show()

"""## Smoking Status Distribution

**Smoking status distribution of both stroke and non-stroke patients**
"""

fig = px.histogram(df, x='smoking_status', color='stroke', barmode='group')
fig.update_layout(bargap=0.2)
fig.show()

df[df['smoking_status']=='never smoked']['stroke'].value_counts(normalize=True)

df[df['smoking_status']=='smokes']['stroke'].value_counts(normalize=True)

df[df['smoking_status']=='formerly smoked']['stroke'].value_counts(normalize=True)

"""## **Correlation between columns**"""

import seaborn as sns
dataplot = sns.heatmap(df.drop('id', axis = 1).corr(), cmap="Blues", annot=True)
dataplot.set_title('Correlation Matrix of all variables')

df.drop('id', axis = 1).corr()['stroke']

"""**As evident by the matrix and the column values of stroke, Age has the highest correelation with stroke of all other features. BMI, on the other hand, has the lowest correlation with the features.**

## **Null values**
"""

df.isnull().sum()

df[df['bmi'].isnull()]['stroke'].value_counts(normalize=True)

"""# **Modeling**

**Dropping the Residence_type featrue as it showed to be irrelevant**
"""

df=df.drop('id',axis=1)
df_num = df.copy()
df_num = df_num.drop('Residence_type', axis = 1)
df_num.isnull().sum()

"""## **Preparing column transformers** 


*   For bmi, we found it's best to use KnnImputer because the meaning and the scale of bmi is dependent on the age. KNN imputer is a suitable choice because it uses information from other features to fill the missing values and BMI is calculated based on gender and age. The same bmi for a child and for an elderly person has different meaning. So, to replace the missing bmi values, we will take into account the neiboghrs to choose a meaningful replacement.
*   Standard scaler is used for numerical features and OneHotEncoding is used for categorical features

*   Binary features are kept as they are.


"""

#Numeric
Imputer = KNNImputer(n_neighbors=5)
#Categorical
ohe = OneHotEncoder(handle_unknown='ignore')

numeric_features = ['age', 'bmi', 'avg_glucose_level']
categorical_features = ['gender', 'ever_married', 'work_type', 'smoking_status']
binary_features = ['hypertension', 'heart_disease']

"""## **Preprocessing**"""

preprocessor = make_column_transformer(
    (make_pipeline(StandardScaler(), Imputer), numeric_features),
    (ohe, categorical_features),
    remainder='passthrough')

#Target feature
X = df_num.drop('stroke', axis = 1)
y = df_num['stroke'].astype(np.uint8)

preprocessor.fit_transform(X)

# The transformd dataframe
transformed_column_names = (numeric_features +
                            preprocessor.named_transformers_['onehotencoder'].
                            get_feature_names_out().tolist() + binary_features)
transformed_column_names

X = pd.DataFrame(preprocessor.fit_transform(X),
                 columns=transformed_column_names)

"""## **Splitting the dataset**"""

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.3,
                                                    random_state=42,
                                                    stratify=y)

"""## **Model Selection and Cross Validation**

We will create a dictionary for our models and to track the models' performance we will keep some metrics stored in a dataframe
"""

models = {
  'L_regressor': LogisticRegression(random_state=42, max_iter=500, class_weight='balanced'),
  'K Nearest Classifier': KNeighborsClassifier(),
  'Decison Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),
  'Stochastic Gradient Descent': SGDClassifier(random_state=42,class_weight='balanced'),
  'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=42),
  'Random Forest Classifier': RandomForestClassifier(random_state=42, class_weight='balanced')
}
models_list =[]
training_accuracy = []
test_accuracy = []
counter = []
precision = []
recall = []
f1 = []

"""### **Dealing with data imbalance**

We will use oversampling on training data only (X_train and y_train)
"""

over = SMOTE(random_state=42)
under = RandomUnderSampler(sampling_strategy=0.5)

X_train_over,y_train_over=over.fit_resample(X_train,y_train)

"""**Now, we will try the following models and test the acuracy of each on the test data:**


1.   Logistic regression
2.   K-Nearest Classifier
3. Decision Trees
4. Stocahstic Gradient Descent Classifier
5. Gradient Boosting Classifier
6. Random Forest Classifier

### Running models
"""

for model, d in models.items():
  print(model)
  print(50*'=')
  #steps = [('pre_processing',preprocessor), ('o', over), ('u', under), ('m', d)]
  
  d.fit(X_train_over, y_train_over)
  predictions = d.predict(X_test)
  acc = accuracy_score(y_test, predictions)
  t_pred = d.predict(X_train_over)
  t_acc = accuracy_score(y_train_over, t_pred)
  count = Counter(predictions)

  #calculating the f Score
  report = precision_recall_fscore_support(y_test,
                                           predictions,
                                           average='binary'    
                                            )
  print(model + ' report:')
  print(classification_report(y_test, predictions))
  print(50*'=')
  
  #Saving the model and its scores
  models_list.append(model)
  training_accuracy.append(t_acc)
  counter.append(count)
  test_accuracy.append(acc)
  precision.append(report[0])
  recall.append(report[1])
  f1.append(report[2])

"""We can then collect all the models' results into one data frame for better visualtization


"""

models_results=pd.DataFrame({'model': models_list,'train_accuracy':t_acc,'test_accuracy':acc,'test_percision':precision,'test_recall':recall,'test_f1':f1,'counter':counter})

models_results

"""Our model aims to predict when a patient is likely to have a stroke. In general it is much riskier to have a false negative predicition than a false positive predicition. In this kind of cases we prefer a higher recall score (lease false negative classifications). After looking at the scores of the chosen 4 models, we see that logisitic regression and stochastic gradient descent achieve the best recall scores but with many false positives. To get more accurate information about the other metrics we will run cross-validation on the selected models.

### Cross Validation

We will use startified k-fold for cross-validating our data. With oversampling the method should only be done on the training data and then get the models' metrics after running on the non-transformed stratified test data. To facilitate our work, we can combine the preprocessing and the modeling in a single pipeline before moving on to tunning the hyper-parameter. The pipeline will have the preprocessor object along with the oversampler and the classifier, which will simplify the modeling process and provide a clean way of representing the data. We will start with the original dataframe df and redefine the preprocessor.
"""

#simple imputer for categorical features
imp_simple=SimpleImputer(strategy='constant')
imp_KNN=KNNImputer(n_neighbors=5, add_indicator=True)
#since that we are working on the original dataset we have to return back the residence type feature
categorical_features = ['gender', 'ever_married', 'work_type','Residence_type' ,'smoking_status']

#redefining the preprocessor
preprocessor = make_column_transformer(
    (make_pipeline(StandardScaler(), imp_KNN), numeric_features),
    (make_pipeline(imp_simple,ohe), categorical_features),
    remainder='passthrough')

y=df['stroke']
X=df.drop(['stroke'],axis=1)

"""We will use the same models dictionary and metrics dataframe for cross-validation"""

models_list =[]
train_accuracy = []
test_accuracy = []
count_list = []
precision = []
recall = []
f1 = []
roc=[]
for model, clf in models.items():
  print(model)
  print(50*'#')

  pipeline = make_pipeline(preprocessor,over, clf)

  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)

  scores = cross_validate(pipeline,
                            X,
                            y,
                            scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'),
                            cv=cv,
                            n_jobs=-1)
  
  models_list.append(model)
  test_accuracy.append(scores['test_accuracy'].mean())
  precision.append(scores['test_precision'].mean())
  recall.append(scores['test_recall'].mean())
  f1.append(scores['test_f1'].mean())
  roc.append(scores['test_roc_auc'].mean())

# Putting it all together in the dataframe
cv_results = pd.DataFrame({
    "model": models_list,
    'test_acc': test_accuracy,
    'test_precision': precision,
    'test_recall': recall,
    'test_f1': f1,
    'test_roc_auc': roc
})
cv_results

"""### **Crossvalidation with manually combining over sampling and undersampling**"""

over = SMOTE(sampling_strategy=0.1)
under = RandomUnderSampler(sampling_strategy=0.5)
#pipeline = imbpipeline(steps=[('o', over), ('u', under)])

models_list = []
test_accuracy = []
precision = []
recall = []
f1 = []
roc = []

for model, d in models.items():
  print(model)
  print(50*'=')
  pipeline = make_pipeline(preprocessor,over,under, d)
  cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)
  scores = cross_validate(pipeline,
                            X,
                            y,
                            scoring=('accuracy', 'precision', 'recall', 'f1', 'roc_auc'),
                            cv=cv,
                            n_jobs=-1)
  models_list.append(model)
  test_accuracy.append(scores['test_accuracy'].mean())
  precision.append(scores['test_precision'].mean())
  recall.append(scores['test_recall'].mean())
  f1.append(scores['test_f1'].mean())
  roc.append(scores['test_roc_auc'].mean())

# Putting it all together in the dataframe
results_cv = pd.DataFrame({
    "model": models_list,
    'test_acc': test_accuracy,
    'test_precision': precision,
    'test_recall': recall,
    'test_f1': f1,
    'test_roc_auc': roc
})
results_cv

"""Based on the results of the crossvalidation we will be preforming the hyperparamater tunning on two models only which are Logistic Regression and Stochastic Gradient Descent

# Hyperparamter Tunning

## Logistic Regression
We will modify our pipeline a little bit first then construct a list of our paramters to run it by the grid search
"""

over=SMOTE()
pipeline=make_pipeline(preprocessor,over,LogisticRegression(max_iter=500))
pipeline.get_params()

param_grid={}
param_grid['logisticregression__C']=[100, 10, 1.0, 0.1, 0.01]
param_grid['logisticregression__penalty']=['l2']
param_grid['logisticregression__solver']=['newton-cg', 'lbfgs', 'liblinear']

grid=GridSearchCV(pipeline,param_grid,cv=cv,scoring='recall',n_jobs=-1)
grid_search=grid.fit(X, y)

grid.best_score_
#best recall score found in the grid search for logistic regression

LR_bestparams=grid.best_params_
LR_bestparams
#best combination of parameters that produced the best recall score

pd.set_option('display.max_colwidth',None)
pd.DataFrame(grid.cv_results_)[['params','mean_test_score','rank_test_score']].sort_values('rank_test_score').head(5)

"""## Stochastic Gradient Descent

Performing the same steps as in LogReg
"""

pipeline=make_pipeline(preprocessor,over, SGDClassifier(random_state=42, class_weight='balanced', max_iter=10000))
pipeline.get_params()

param_grid={}
param_grid['sgdclassifier__loss']=['hinge', 'log','modified_huber']
param_grid['sgdclassifier__penalty']=['l1','l2']
param_grid['sgdclassifier__alpha']=[0.0001,0.001,0.01,0.1,1]

grid=GridSearchCV(pipeline,param_grid,cv=cv,scoring='recall',n_jobs=-1)
grid_search=grid.fit(X, y)

grid.best_score_
#best recall score found in the grid search for SGD

SGD_bestparams=grid.best_params_
SGD_bestparams
#best combination of parameters that produced the best recall score

pd.set_option('display.max_colwidth',None)
pd.DataFrame(grid.cv_results_)[['params','mean_test_score','rank_test_score']].sort_values('rank_test_score').head(5)

"""# Creating Models
We will then construct pipelines for our two chosen models and fit the training data using them and look at our predictions after tunning and optimizing the hyperparameters.
"""

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

over=SMOTE(random_state=42)
LR_pipeline=make_pipeline(preprocessor,over,LogisticRegression(C=0.01,penalty='l2',solver='liblinear',max_iter=500))
SGD_pipeline=make_pipeline(preprocessor,over,SGDClassifier(loss='hinge',alpha=0.1,penalty='l1',class_weight='balanced',random_state=42,max_iter=1000))

LR_pipeline.fit(X_train,y_train)
LR_predictions=LR_pipeline.predict(X_test)
print(classification_report(y_test,LR_predictions))

SGD_pipeline.fit(X_train,y_train)
SGD_predictions=SGD_pipeline.predict(X_test)
print(classification_report(y_test,SGD_predictions))

"""## **Confusoion Matrix and ROC Curve**

### Logsitic Regression
"""

disp= ConfusionMatrixDisplay.from_estimator(LR_pipeline,X_test,y_test)

FalsePositiveRate_LR, TruePositiveRate_LR, _ = roc_curve(y_test, LR_pipeline.predict_proba(X_test)[:,1])

plt.figure(figsize=(12,8));

plt.plot(FalsePositiveRate_LR, TruePositiveRate_LR);
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')
plt.plot([0, 1], [0, 1], lw=3, linestyle='--');

print('Auc : ', auc(FalsePositiveRate_LR, TruePositiveRate_LR))

"""### Stochastic Gradient Descent"""

disp= ConfusionMatrixDisplay.from_estimator(SGD_pipeline,X_test,y_test)

FalsePositiveRate_LR, TruePositiveRate_LR, _ = roc_curve(y_test, SGD_pipeline.decision_function(X_test))

plt.figure(figsize=(12,8));

plt.plot(FalsePositiveRate_LR, TruePositiveRate_LR);
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC curve')
plt.plot([0, 1], [0, 1], lw=3, linestyle='--');

print('Auc : ', auc(FalsePositiveRate_LR, TruePositiveRate_LR))

"""## Plotting Feature Importance _ Logistic Regression

Extracting features names and theri corresponding coefficients from the pipeline
"""

coefs = LR_pipeline['logisticregression'].coef_.flatten()
ohe_features=preprocessor.named_transformers_['pipeline-2']['onehotencoder'].get_feature_names_out().tolist()
features_transformed=(numeric_features+ohe_features+binary_features)

f_imp=zip(features_transformed,coefs)
f_imp_df=pd.DataFrame(f_imp,columns=["feature","Importance"])
f_imp_df["absolute"] = f_imp_df["Importance"].apply(lambda x: abs(x))
f_imp_df = f_imp_df.sort_values("absolute", ascending=False)

fig, ax = plt.subplots(1, 1, figsize=(12, 7))
sns.barplot(x="feature", y="absolute", data=f_imp_df.head(20))

ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)
ax.set_title("Top 20 Features", fontsize=26)
ax.set_ylabel("Coef(feature Importance)", fontsize=19)
ax.set_xlabel("Feature Name", fontsize=19)

"""# Final Model and Making Predictions

After comparing both models, we will pick one of them and run it on the full dataset. We can see that logistic regression performed on average better than SGD across all the tests. We will choose the logistic regression model and tune it with our hyperparameters
"""

imp_simple=SimpleImputer(strategy='constant')
imp_KNN=KNNImputer(n_neighbors=5)
ohe = OneHotEncoder(handle_unknown='ignore')
#redefining the preprocessor
preprocessor = make_column_transformer(
    (make_pipeline(StandardScaler(), imp_KNN), numeric_features),
    (make_pipeline(imp_simple,ohe), categorical_features),
    remainder='passthrough')
over = SMOTE(random_state=42)

LR_pipeline=make_pipeline(preprocessor,over,LogisticRegression(C=0.01,penalty='l2',solver='liblinear',max_iter=500))

LR_pipeline.fit(X, y)
LR_predictions = LR_pipeline.predict(X)
print(classification_report(y, LR_predictions))

disp = ConfusionMatrixDisplay.from_estimator(LR_pipeline,X,y, display_labels=LR_pipeline.classes_);

"""# Predicting Stroke Risk

Finally after choosing the model and optimzing it to achieve our target we can write a function that takes the patient's data and calculate the probability (risk) of them getting stoke
"""

def stroke_risk(patient_data):
  ''' The input has to be a dictionary with keys similar to the features' names
  of the dataset and the output is going to be the probability of having stroke '''
  input = pd.DataFrame(patient_data)
  model_prediction=LR_pipeline.predict_proba(input)
  print('The predicted probability of Stroke', round(model_prediction[0][1],2)*100)
  return model_prediction

"""We will test our model on a 49 year old male who has history of heart disease, never smoked, married, has relatively high average glucose level and normal BMI"""

Patient1 = {
    'gender': ['Male'],
    'age': [49.0],
    'hypertension': [0],
    'heart_disease': [1],
    'ever_married': ['Yes'],
    'work_type': ['Govt_job'],
    'Residence_type': ['Rural'],
    'avg_glucose_level': [130.00],
    'bmi': [24.6],
    'smoking_status': ['never smoked']
}
stroke_risk(Patient1)

"""Predictions show that this man has 28% chance of having stroke which is not very high but also definetly not low!"""